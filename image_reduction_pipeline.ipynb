{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Reduction Pipeline Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Description:</b> This pipeline is meant to be part of a three-step process for performing timing analysis on observations of a target time-variable object. The first step of the process was to reduce FITS files of the object by debiasing, dark-subtracting, and flat-fielding. For the pipeline corresponding to the first step, refer to <i>image_reduction_pipeline.ipynb</i> (this file). The second step of the process is to perform aperture photometry on the reduced FITS files and extract the apparent magnitude(s) for the desired object(s). For the pipeline corresponding to the second step, refer to <i>aperture_photometry_pipeline.ipynb</i>. The third step of this process is to perform the timing analysis on the extracted magnitudes themselves. For the pipeline corresponding to the third step, refer to <i>timing_analysis_pipeline.ipynb</i>.\n",
    "\n",
    "<b>This Jupyter Notebook file will perform the first step of this process: reduction of the unprocessed FITS files</b>. This pipeline is meant to debias, dark-subtract, and flatten all your fits files of an astronomical object. \n",
    "\n",
    "<b>Benefits and Limitations:</b> You can run this pipeline on directories containing multiple bandpasses, and the pipeline will automatically sort out the fits files by bandpass and reduce them accordingly. However, the limitation of this pipeline is that it does not automatically sort out fits file by night, so you will have to feed in your image files, bias files, dark files, and flat files separately by night into this pipeline. Another limitation is that it does not automatically trim the science files; you will manually have to do that separately before feeding in your fits files into this pipeline.\n",
    "\n",
    "<b>Output:</b> This pipeline will automatically create local subdirectories within your current directory. Each subdirectory corresponds to a different bandpass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Run This File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The command line arguments to run this Python file is written below:\n",
    "#### python image_reduction_pipeline.py dir_raw_fits dir_bias dir_flats dir_darks=None\n",
    "<ul>\n",
    "    <li>The first argument is the name of this python file (image_reduction_pipeline.py).\n",
    "    <li>The second argument is the PATH of the directory containing all your raw fits files.\n",
    "    <li>The third argument is the PATH of the directory containing all your bias files.\n",
    "    <li>The fourth argument is the PATH of the directory containing all your flat files.\n",
    "    <li>The (optional) fifth argument is the PATH of the directory containing all your dark files.\n",
    "</ul>\n",
    "\n",
    "### Assumptions Implicit in This Pipeline\n",
    "<ul>\n",
    "    <li>This pipeline works assuming that all the headers of your fits files are properly labeled with their bandpasses. \n",
    "    <li>This pipeline also assumes that your fits files are directly within the directories fed into the command line, instead of being separated within subdirectories. The advantage of this option is that you can put all the fits files you don't want involved in the image reduction process into a subdirectory of the directories you feed into the command line, and the pipeline will not read in these \"undesired\" fits files.\n",
    "    <li>This pipeline assumes that you are only inputting data for one night into the command line (as opposed to inputting data for multiple nights into the command line). This is because the pipeline cannot separate fits files by night when reducing the image files.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages and Setting Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages and setting initial conditions\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from astropy.io import fits\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "#os.chdir(\"test_dirs_Dec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Files from Command Line Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just for testing purposes\n",
    "while len(sys.argv) < 4:\n",
    "    sys.argv.append(\"\")\n",
    "sys.argv[1] = \"C:\\\\Users\\\\baske\\\\Downloads\\\\A0620 Data\\\\test_dirs_Dec\\\\raw_fits_dir\"\n",
    "sys.argv[2] = \"C:\\\\Users\\\\baske\\\\Downloads\\\\A0620 Data\\\\test_dirs_Dec\\\\bias_dir\"\n",
    "sys.argv[3] = \"C:\\\\Users\\\\baske\\\\Downloads\\\\A0620 Data\\\\test_dirs_Dec\\\\flats_dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a helper function that returns whether a file's extension is '.fits' or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_fits_file_extension(filename):\n",
    "    '''A function that returns whether a file's extension is '.fits' or not.'''\n",
    "    return filename.split(\".\")[-1] == \"fits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the directories and filenames for the raw fits files, the bias files, the flat files, and (optionally) the dark files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(sys.argv) == 1:\n",
    "    raise TypeError(\"timing_analysis_pipeline.py missing three positional arguments: 'raw_fits_dir', 'bias_dir', and 'flats_dir'\")\n",
    "if len(sys.argv) == 2:\n",
    "    raise TypeError(\"timing_analysis_pipeline.py missing two positional arguments: 'bias_dir', and 'flats_dir'\")\n",
    "if len(sys.argv) == 3:\n",
    "    raise TypeError(\"timing_analysis_pipeline.py missing one positional argument: 'flats_dir'\")\n",
    "\n",
    "# If sys.argv has four indices, that means the command line inputted in the raw_fits_dir, bias_dir, and the flats_dir.\n",
    "raw_fits_filenames = list(filter(has_fits_file_extension, os.listdir(sys.argv[1])))\n",
    "bias_filenames = list(filter(has_fits_file_extension, os.listdir(sys.argv[2])))\n",
    "flat_filenames = list(filter(has_fits_file_extension, os.listdir(sys.argv[3])))\n",
    "\n",
    "dark_filenames = []\n",
    "# If sys.argv has five indices, that means the command line inputted in the raw_fits_dir, bias_dir, flats_dir, and the darks_dir.\n",
    "if len(sys.argv) == 5:\n",
    "    dark_filenames = list(filter(os.path.isfile, os.listdir(sys.argv[4])))\n",
    "    \n",
    "# If sys.argv has more than five indices, that means the command line inputted in too many parameters.\n",
    "if len(sys.argv) > 5:\n",
    "    raise TypeError(f\"timing_analysis_pipeline.py takes from 3 to 4 positional arguments, but {len(sys.argv)} were given\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the files in the current directory and obtaining their data and headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fits_headers = []\n",
    "raw_fits_data = []\n",
    "for fits_filename in raw_fits_filenames:\n",
    "    with fits.open(f\"{sys.argv[1]}/{fits_filename}\") as hdu:\n",
    "        raw_fits_headers.append(hdu[0].header)\n",
    "        raw_fits_data.append(hdu[0].data)\n",
    "\n",
    "bias_headers = []\n",
    "bias_data = []\n",
    "for bias_filename in bias_filenames:\n",
    "    with fits.open(f\"{sys.argv[2]}/{bias_filename}\") as hdu:\n",
    "        bias_headers.append(hdu[0].header)\n",
    "        bias_data.append(hdu[0].data)\n",
    "\n",
    "flat_headers = []\n",
    "flat_data = []\n",
    "for flat_filename in flat_filenames:\n",
    "    with fits.open(f\"{sys.argv[3]}/{flat_filename}\") as hdu:\n",
    "        flat_headers.append(hdu[0].header)\n",
    "        flat_data.append(hdu[0].data)\n",
    "\n",
    "dark_headers = []\n",
    "dark_data = []\n",
    "for dark_filename in dark_filenames:\n",
    "    with fits.open(f\"{sys.argv[4]}/{dark_filename})\") as hdu:\n",
    "        dark_headers.append(hdu[0].header)\n",
    "        dark_data.append(hdu[0].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Files by Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates list of all the filters in the fits files of the raw_fits_dir directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fits_filters = np.unique(np.array([raw_fits_headers[i][\"FILTER\"] for i in range(len(raw_fits_headers))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separates out the data for the raw fits files, bias files, flat files, and dark files (if any) into their respective filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_fits_dict = {}\n",
    "#bias_dict = {}\n",
    "flat_dict = {}\n",
    "#dark_dict = {}\n",
    "for bandpass in raw_fits_filters:\n",
    "    raw_fits_data_per_bandpass = []\n",
    "    for i in range(len(raw_fits_headers)):\n",
    "        if raw_fits_headers[i][\"FILTER\"] == bandpass:\n",
    "            raw_fits_data_per_bandpass.append([raw_fits_filenames[i], raw_fits_data[i]])\n",
    "    raw_fits_dict[bandpass] = raw_fits_data_per_bandpass\n",
    "\n",
    "    # Apparently for bias files, you don't need to separate them by bandpass (?)\n",
    "    #bias_data_per_bandpass = []\n",
    "    #for i in range(len(bias_headers)):\n",
    "    #    if bias_headers[i][\"FILTER\"] == bandpass:\n",
    "    #        bias_data_per_bandpass.append(bias_data[i])\n",
    "    #bias_dict[bandpass] = bias_data_per_bandpass\n",
    "    \n",
    "    flat_data_per_bandpass = []\n",
    "    for i in range(len(flat_headers)):\n",
    "        if flat_headers[i][\"FILTER\"] == bandpass:\n",
    "            flat_data_per_bandpass.append([flat_filenames, flat_data[i]])\n",
    "    flat_dict[bandpass] = flat_data_per_bandpass\n",
    "    \n",
    "    # You might not need to separate dark files by bandpass either\n",
    "    #dark_data_per_bandpass = []\n",
    "    #for i in range(len(dark_headers)):\n",
    "    #    if dark_headers[i][\"FILTER\"] == bandpass:\n",
    "    #        dark_data_per_bandpass.append(dark_data[i])\n",
    "    #dark_dict[bandpass] = dark_data_per_bandpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Bias and Dark Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median combining the bias and darks, and subtracting the median-combined bias from the median-combined dark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently for bias and dark files, you don't need to separate them by bandpass (?)\n",
    "comb_bias = np.median(np.array(bias_data), axis=0) # Median combines all the bias files for a specific bandpass\n",
    "comb_bias_median_per_row = np.median(comb_bias, axis=1) # Takes the median of each row in the median-combined bias file, and puts the list of medians (of each row) into a np.array\n",
    "\n",
    "# Suppresses any RuntimeWarnings that may arise from combining dark files. This is a possibility because if you don't\n",
    "#   input a dark_dir into the command line, np.median() will give back a RuntimeWarning.\n",
    "def fxn():\n",
    "    warnings.warn(\"mean of empty slice\", RuntimeWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "    comb_dark = np.median(np.array(dark_data), axis=0) # Median combines all the dark files for a specific bandpass\n",
    "    bias_subtracted_dark = np.subtract(comb_dark,np.reshape(comb_bias_median_per_row, (len(comb_bias_median_per_row),1))) # Subtracts the median of each row of the combined bias file from each row of the combined dark file\n",
    "    comb_dark_median_per_row = np.median(bias_subtracted_dark, axis=1) # Takes the median of each row in the debiased combined dark file, and puts the list of medians (of each row) into a np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the combined bias and dark file into current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes combined bias file to current directory.\n",
    "hdu = fits.PrimaryHDU(comb_bias)\n",
    "hdu.writeto(f\"{os.getcwd()}/BiasComb.fits\", overwrite=True)\n",
    "\n",
    "# If any dark files were fed into the command line, writes the combined dark file to current directory\n",
    "if not np.isnan(comb_dark):\n",
    "    hdu = fits.PrimaryHDU(bias_subtracted_dark)\n",
    "    hdu.writeto(f\"{os.getcwd()}/DarkCombDebiased.fits\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Flat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracting the flat files by both the combined bias and debiased combined dark file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_flats_dict = {}\n",
    "for bandpass in list(flat_dict.keys()):\n",
    "    flat_bandpass_data = []\n",
    "    for flat_filename, flat_data in flat_dict[bandpass]:\n",
    "        # Debiases each flat file\n",
    "        bias_subtracted_flat = np.subtract(flat_data,np.reshape(comb_bias_median_per_row, (len(comb_bias_median_per_row),1))) # Subtracts the median of each row of the combined bias file from each row of each flat file\n",
    "        \n",
    "        # If dark files are inputted into the command line, then subtract the debiased combined dark from each debiased flat file. Otherwise, just use the debiased flat file in further image processing steps.\n",
    "        # Finds the mean of each flat and normalizes the flat by its corresponding mean\n",
    "        if dark_filenames == []:\n",
    "            flat_mean = np.mean(bias_subtracted_flat)\n",
    "            flat_normalized = bias_subtracted_flat / flat_mean\n",
    "        else:\n",
    "            dark_and_bias_subtracted_flat = np.subtract(bias_subtracted_flat,np.reshape(comb_dark_median_per_row, (len(comb_dark_median_per_row),1))) # Subtracts the median of each row of the debiased combined dark file from each row of each flat file\n",
    "            flat_mean = np.mean(dark_and_bias_subtracted_flat)\n",
    "            flat_normalized = dark_and_bias_subtracted_flat / flat_mean\n",
    "        flat_bandpass_data.append(flat_normalized)\n",
    "    combined_flat = np.median(flat_bandpass_data, axis=0)\n",
    "    combined_flats_dict[bandpass] = combined_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the processed flat files into current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes combined bias file to current directory.\n",
    "for bandpass in combined_flats_dict.keys():\n",
    "    hdu = fits.PrimaryHDU(combined_flats_dict[bandpass])\n",
    "    hdu.writeto(f\"{os.getcwd()}/{bandpass}_FlatCombProcessed.fits\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Science Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtracts the science images by both the combined bias and debiased combined dark file, and then flattens the science file by its corresponding flat file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debiases, dark-subtracts, and flattens the science files\n",
    "flattened_sci_dict = {}\n",
    "for bandpass in list(raw_fits_dict.keys()):\n",
    "    flattened_sci_per_bandpass_data = []\n",
    "    for sci_filename, sci_data in raw_fits_dict[bandpass]:\n",
    "        bias_subtracted_sci = np.subtract(sci_data,np.reshape(comb_bias_median_per_row, (len(comb_bias_median_per_row),1))) # Subtracts the median of each row of the combined bias file from each row of each science file\n",
    "        # If dark files are inputted into the command line, then subtract the debiased combined dark from each debiased flat file. Otherwise, just use the debiased flat file in further image processing steps.\n",
    "        if dark_filenames == []:\n",
    "            flattened_sci = bias_subtracted_sci / combined_flats_dict[bandpass] # Flattens the science file\n",
    "        else:\n",
    "            dark_and_bias_subtracted_sci = np.subtract(bias_subtracted_sci,np.reshape(comb_dark_median_per_row, (len(comb_dark_median_per_row),1))) # Subtracts the median of each row of the debiased combined dark file from each row of each science file\n",
    "            flattened_sci = dark_and_bias_subtracted_sci / combined_flats_dict[bandpass] # Flattens the science file\n",
    "        flattened_sci_per_bandpass_data.append([sci_filename, flattened_sci])\n",
    "    flattened_sci_dict[bandpass] = flattened_sci_per_bandpass_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing all the processed science files into local directories labeled by bandpass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bandpass in list(flattened_sci_dict.keys()):\n",
    "    bandpass_path_name = os.path.join(os.getcwd(), bandpass)\n",
    "    try:\n",
    "        os.mkdir(bandpass_path_name)\n",
    "        for i in range(len(flattened_sci_dict[bandpass])):\n",
    "            src_file_path = f\"{sys.argv[1]}/{flattened_sci_dict[bandpass][i][0]}\"\n",
    "            out_file_path = f\"{bandpass_path_name}/proc_{flattened_sci_dict[bandpass][i][0]}\"\n",
    "            shutil.copy(src_file_path, out_file_path)\n",
    "            with fits.open(out_file_path, mode=\"update\") as hdu:\n",
    "                hdu[0].header[\"WCSNAME\"] = \"REDUCED\"\n",
    "                hdu[0].data = flattened_sci_dict[bandpass][i][1]\n",
    "    except:\n",
    "        for i in range(len(flattened_sci_dict[bandpass])):\n",
    "            src_file_path = f\"{sys.argv[1]}/{flattened_sci_dict[bandpass][i][0]}\"\n",
    "            out_file_path = f\"{bandpass_path_name}/proc_{flattened_sci_dict[bandpass][i][0]}\"\n",
    "            shutil.copy(src_file_path, out_file_path)\n",
    "            with fits.open(out_file_path, mode=\"update\") as hdu:\n",
    "                hdu[0].header[\"WCSNAME\"] = \"REDUCED\"\n",
    "                hdu[0].data = flattened_sci_dict[bandpass][i][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
